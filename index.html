<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="utf-8">
  <meta name="description" content="SLAM-Dunk-Prometheus: 실시간 LiDAR-IMU SLAM과 3D 세그멘테이션">
  <meta name="keywords" content="SLAM, LiDAR, IMU, 3D 세그멘테이션, 컴퓨터 비전, 로보틱스">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SLAM-Dunk-Prometheus</title>

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

  <!-- Bulma CSS -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  
  <!-- Custom CSS -->
  <link rel="stylesheet" href="style.css">
</head>

<body>
  <!-- 제목 섹션 -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="publication-title">SLAM-Dunk-Prometheus</h1>
            <p class="publication-subtitle">iPhone LiDAR-IMU SLAM & Semantic Segmentation</p>
            
            <div class="publication-authors">
              <span class="author-block">
                Prometheus Team
              </span>
            </div>

            <!-- 링크 -->
            <div class="publication-links">
              <span class="link-block">
                <a href="#demo-panel" class="button is-light is-rounded">
                  <span class="icon">
                    <i class="fas fa-image"></i>
                  </span>
                  <span>데모 패널</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/SLAM-Dunk-Prometheus" class="button is-light is-rounded">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>GitHub</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#demo-video" class="button is-light is-rounded">
                  <span class="icon">
                    <i class="fas fa-video"></i>
                  </span>
                  <span>데모 영상</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#pipeline-overview" class="button is-light is-rounded">
                  <span class="icon">
                    <i class="fas fa-diagram-project"></i>
                  </span>
                  <span>파이프라인</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- 개요 -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title">개요</h2>
          <div class="content has-text-justified">
            <p>
              SLAM-Dunk-Prometheus는 iPhone의 LiDAR·IMU·RGB 카메라만을 사용해 실내 공간을 3D로 재구성하고,
              그 위에 의미론적 이해(semantic understanding)를 결합해 '공간이 무엇인지'까지 추론하는 end-to-end 파이프라인입니다.
            </p>
            <p>
              Depth+IMU 기반 Geometry SLAM으로 카메라 궤적을 안정적으로 복원한 뒤 TSDF fusion으로 정제된 3D 맵(mesh/point cloud)을 생성하고,
              Mosaic3D(geometry-only)와 OpenScene(open-vocabulary)를 결합해 구조물과 물체를 함께 인식하는 semantic map을 구축합니다.
            </p>
            <p>
              최종 결과는 웹 기반 인터랙티브 데모(viser)로 시각화되며, 사용자는 “wall”, “bed”, “Where can I walk?” 같은 텍스트 질의로
              3D 공간에서 관련 영역을 즉시 탐색할 수 있습니다.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- 데모 패널 -->
  <section class="section" id="demo-panel">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title">데모 패널</h2>
          <div class="image-container">
            <img src="panel/slamdunk_panel.png" alt="SLAM-Dunk 데모 패널" class="panel-image" />
            <p style="font-size: 0.9rem; color: #888; margin-top: 1rem;">
              실시간 SLAM 결과 및 세그멘테이션 시각화
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- 시스템 아키텍처 -->
  <section class="section" style="background-color: #fafafa;">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title has-text-centered">시스템 모듈</h2>
          
          <div class="module-card">
            <h3>SLAM 파이프라인</h3>
            <div class="content">
              <p>
                StrayScanner 데이터셋에 최적화된 실시간 LiDAR-IMU SLAM 시스템입니다. 
                주요 기능은 다음과 같습니다.
              </p>
              <ul>
                <li><strong>센서 융합:</strong> 고주파 LiDAR와 IMU 센서 데이터를 융합하여 정확한 위치 추정</li>
                <li><strong>포즈 추정:</strong> 포즈 추정 및 궤적 최적화를 통한 정밀한 로봇 위치 추적</li>
                <li><strong>포인트 클라우드 처리:</strong> 효율적인 포인트 클라우드 처리 및 3D 맵 구축</li>
                <li><strong>데이터 전처리:</strong> 포괄적인 데이터 전처리 및 후처리 도구 제공</li>
              </ul>
              <p style="margin-top: 1rem;">
                <strong>기술적 특징:</strong> 
                depth 맵과 confidence 맵을 활용하여 높은 정확도의 3D 맵을 생성합니다. 
                post_processor.py를 통해 데이터 통합 및 후처리를 수행할 수 있습니다.
              </p>
              <div class="tag-container">
                <span class="tag is-info">Python</span>
                <span class="tag is-info">LiDAR</span>
                <span class="tag is-info">IMU</span>
                <span class="tag is-info">포인트 클라우드</span>
              </div>
            </div>
          </div>

          <div class="module-card">
            <h3>3D 세그멘테이션</h3>
            <div class="content">
              <p>
                딥러닝 모델을 융합한 3D Semantic Segmentation 시스템입니다. 
                주요 기능은 다음과 같습니다.
              </p>
              <ul>
                <li><strong>Mosaic3D 통합:</strong> Mosaic3D 모델을 활용한 고정밀 3D 세그멘테이션</li>
                <li><strong>OpenScene 지원:</strong> OpenScene 모델을 통한 다양한 장면 이해</li>
                <li><strong>대화형 시각화:</strong> 세그멘테이션된 장면을 위한 대화형 시각화 도구</li>
              </ul>
              <p style="margin-top: 1rem;">
                <strong>기술적 특징:</strong> 본 모듈은 convert_stray.py를 통해 StrayScanner 데이터를 
                세그멘테이션 모델에 적합한 형식으로 변환합니다. Mosaic3D와 OpenScene 모델을 모두 지원하여 
                다양한 환경에서 높은 정확도의 세그멘테이션을 제공합니다.
              </p>
              <div class="tag-container">
                <span class="tag is-success">Python</span>
                <span class="tag is-success">딥러닝</span>
                <span class="tag is-success">3D 비전</span>
                <span class="tag is-success">의미론적 세그멘테이션</span>
                <span class="tag is-success">Mosaic3D</span>
                <span class="tag is-success">OpenScene</span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- 데모 영상 -->
  <section class="section" id="demo-video">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title">데모 영상</h2>
          <div class="video-container">
            <video controls>
              <source src="video/Seg_demo.mp4" type="video/mp4">
              브라우저가 비디오 태그를 지원하지 않습니다.
            </video>
          </div>
          <p style="font-size: 0.9rem; color: #888; margin-top: 1rem; text-align: center;">
            SLAM과 세그멘테이션이 작동하는 전체 파이프라인 시연
          </p>
        </div>
      </div>
    </div>
  </section>

  <!-- 핵심 개념 -->
  <section class="section" style="background-color: #fafafa;">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title has-text-centered">핵심 개념</h2>
          <div class="content has-text-justified">
            
            <h3 style="color: #667eea; margin-top: 2rem;">SLAM이란?</h3>
            <p>
              SLAM(Simultaneous Localization And Mapping)은 로봇이나 자율주행 차량이 미지의 환경에서 
              동시에 자신의 위치를 파악하고 주변 환경의 지도를 작성하는 기술입니다. 본 프로젝트에서는 
              LiDAR(Light Detection And Ranging) 센서와 IMU(Inertial Measurement Unit)를 융합하여 
              3차원 공간에서의 정밀한 위치 추정과 맵 구축을 수행합니다.
            </p>

            <h3 style="color: #667eea; margin-top: 2rem;">LiDAR-IMU 센서 융합</h3>
            <p>
              LiDAR는 레이저를 사용하여 주변 환경까지의 거리를 측정하는 센서로, 정밀한 3D 포인트 클라우드 
              데이터를 제공합니다. IMU는 가속도계와 자이로스코프를 포함하여 로봇의 움직임과 방향을 측정합니다. 
              이 두 센서를 융합함으로써 각 센서의 장점을 살리고 단점을 보완하여 더 정확하고 안정적인 
              위치 추정이 가능합니다.
            </p>

            <h3 style="color: #667eea; margin-top: 2rem;">3D 의미론적 세그멘테이션</h3>
            <p>
              3D 의미론적 세그멘테이션은 3차원 포인트 클라우드의 각 점에 의미론적 레이블(예: 벽, 바닥, 가구, 
              사람 등)을 할당하는 작업입니다. 본 프로젝트는 Mosaic3D와 OpenScene 같은 최신 딥러닝 모델을 
              활용하여 실시간으로 3D 장면을 이해하고 분류합니다. 이를 통해 로봇은 단순히 장애물의 위치뿐만 
              아니라 그것이 무엇인지까지 이해할 수 있습니다.
            </p>

            <h3 style="color: #667eea; margin-top: 2rem;">StrayScanner 데이터셋</h3>
            <p>
              StrayScanner는 모바일 기기를 사용하여 수집한 실내 환경 데이터셋입니다. RGB 영상, depth 맵, 
              IMU 데이터, odometry 정보를 포함하며, 본 프로젝트는 이 데이터셋을 활용하여 실시간 SLAM 및 
              세그멘테이션 알고리즘을 개발하고 검증합니다.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- 파이프라인 설명 (기술적 특징 다음) -->
  <section class="section" id="pipeline-overview" style="background-color: #fafafa;">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title has-text-centered">파이프라인 설명</h2>

          <!-- Pipeline Overview: Goal / Why / Inputs -->
          <div class="module-card">
            <h3>Pipeline Overview</h3>
            <div class="content">
              <div class="columns is-variable is-6">
                <div class="column">
                  <p><strong>Goal</strong></p>
                  <ul>
                    <li>iPhone LiDAR + IMU + Camera라는 제한된 센서만으로 실내 공간을 3D로 재구성
                    <li>공간/물체의 의미를 추론</li>
                    <li>“여긴 뭐야?”, “어디를 걸어갈 수 있어?” 같은 질문에 반응하는 기능 구현</li>
                  </ul>
                </div>
                <div class="column">
                  <p><strong>Why this matters</strong></p>
                  <ul>
                    <li>고가의 360° LiDAR 없이 동작하는 최근 가정용 로봇의 방향성과 유사</li>
                    <li>iPhone LiDAR는 시야각/해상도/노이즈 측면에서 어려운 센서 조건</li>
                    <li>해당 조건에서 SLAM + semantic understanding을 끝까지 연결한 레퍼런스가 매우 제한적</li>
                  </ul>
                </div>
              </div>

              <p style="margin-top: 0.75rem;"><strong>Input sensors</strong></p>
              <div class="tags">
                <span class="tag is-link is-light">LiDAR depth (low-res, narrow FoV)</span>
                <span class="tag is-link is-light">IMU (gyro/accel)</span>
                <span class="tag is-link is-light">RGB camera (appearance + language query)</span>
              </div>
            </div>
          </div>

          <!-- Steps -->
          <div class="module-card">
            <h3>Step-by-step Pipeline</h3>
            <div class="content">

              <article class="message is-info is-light">
                <div class="message-header">
                  <p>Step 0. iPhone Scan Output</p>
                </div>
                <div class="message-body">
                  <p><strong>What you get after a quick scan</strong></p>
                  <ul>
                    <li>Depth frames: 각 픽셀의 거리 정보</li>
                    <li>RGB frames: 일반 컬러 영상 프레임</li>
                    <li>IMU stream: 회전/가속도 변화 기록</li>
                    <li>Camera intrinsics: 카메라 내부 파라미터</li>
                  </ul>
                  <p style="margin-top: 0.75rem;"><strong>Key point</strong></p>
                  <ul>
                    <li>각 프레임이 서로 어떤 위치 관계인지 모르는 상태</li>
                    <li>프레임을 단순히 쌓으면 3D가 뒤틀리고 누적 오차가 커짐</li>
                    <li>다음 SLAM 단계에서 '카메라가 어떻게 움직였는지'를 복원해야 함</li>
                  </ul>
                </div>
              </article>

              <article class="message is-link is-light">
                <div class="message-header">
                  <p>Step 1. Geometry SLAM (Depth + IMU)</p>
                </div>
                <div class="message-body">
                  <p><strong>Purpose</strong></p>
                  <ul>
                    <li>Depth/IMU만으로 카메라 이동을 추정</li>
                    <li>프레임들을 하나의 좌표계로 정합 가능한 상태로 만드는 단계</li>
                    <li>3D 재구성에 필요한 pose(경로) 획득</li>
                  </ul>

                  <p style="margin-top: 0.75rem;"><strong>Core components</strong></p>
                  <ul>
                    <li><strong>ICP (frame alignment):</strong> 연속 프레임의 3D 점들을 최대한 겹치게 맞춤 → 상대 이동 추정</li>
                    <li><strong>Pose Graph (global consistency):</strong> ICP 관계를 그래프로 묶어 전체 경로 구성 및 전역 일관성 유지</li>
                    <li><strong>Loop Closure (drift correction):</strong> 재방문 시 같은 장소 제약을 추가해 누적 오차(drift) 보정</li>
                  </ul>

                  <p style="margin-top: 0.75rem;"><strong>Why it’s hard with iPhone LiDAR</strong></p>
                  <ul>
                    <li>좁은 시야각, 낮은 해상도, 높은 노이즈</li>
                    <li>360° LiDAR 기반 SLAM처럼 풍부한 기하 정보가 없음</li>
                    <li>파라미터/구성요소를 직접 실험하며 안정화가 필요</li>
                  </ul>

                  <p style="margin-top: 0.75rem;"><strong>Output</strong></p>
                  <ul>
                    <li>Refined camera trajectory (poses)</li>
                  </ul>
                <!-- Visual outputs (image) -->
                <div class="columns is-variable is-5" style="margin-top: 1.2rem;">
                  <div class="column is-12">
                    <figure class="image pipeline-figure">
                      <img src="image/step1_pose.jpg" alt="refined_pose" class="pipeline-img" />
                      <figcaption class="pipeline-caption">refined_pose</figcaption>
                    </figure>
                  </div>
                </div>
                </div>
              </article>

              <article class="message is-success is-light">
                <div class="message-header">
                  <p>Step 2. 3D Reconstruction</p>
                </div>
                <div class="message-body">
                  <p><strong>Purpose</strong></p>
                  <ul>
                    <li>SLAM으로 얻은 경로를 기준으로 depth를 하나의 좌표계로 통합</li>
                    <li>실내 공간을 3D mesh / point cloud로 생성</li>
                  </ul>

                  <p style="margin-top: 0.75rem;"><strong>What happens</strong></p>
                  <ul>
                    <li><strong>Depth filtering:</strong> 신뢰도 낮은 픽셀 제거, outlier 제거</li>
                    <li><strong>TSDF fusion:</strong> 여러 프레임 depth를 안정적으로 통합, 표면을 매끄럽게 복원</li>
                    <li><strong>Post-processing:</strong> 작은 조각/이상치 제거, 표면 정리 및 smoothing</li>
                  </ul>

                  <p style="margin-top: 0.75rem;"><strong>Output</strong></p>
                  <ul>
                    <li>Refined mesh (3D surface)</li>
                    <li>Refined point cloud (3D points)</li>
                  </ul>

                <!-- Visual outputs (images) -->
                <div class="columns is-variable is-5" style="margin-top: 1.2rem;">
                  <div class="column">
                    <figure class="image pipeline-figure">
                      <img src="image/step2_mesh.jpg" alt="refined_mesh" class="pipeline-img" />
                      <figcaption class="pipeline-caption">refined_mesh</figcaption>
                    </figure>
                  </div>
                  <div class="column">
                    <figure class="image pipeline-figure">
                      <img src="image/step2_pc.jpg" alt="refined_pointcloud" class="pipeline-img" />
                      <figcaption class="pipeline-caption">refined_pointcloud</figcaption>
                    </figure>
                  </div>
                </div>
                </div>
              </article>

              <article class="message is-warning is-light">
                <div class="message-header">
                  <p>Step 3. Geometry-only Segmentation (Mosaic3D)
                </div>
                <div class="message-body">
                  <p><strong>Purpose</strong></p>
                  <ul>
                    <li><strong>RGB 없이 형상 정보만으로</strong> semantic segmentation 시도</li>
                    <li><strong>바닥/벽/천장처럼 구조적으로 확실한 요소를 안정적으로 확보</strong></li>
                  </ul>

                  <p style="margin-top: 0.75rem;"><strong>Strength</strong></p>
                  <ul>
                    <li>큰 구조물(바닥/벽/천장)은 비교적 안정적으로 분리 가능</li>
                    <li>조명/텍스처 변화에 영향을 덜 받음</li>
                  </ul>

                  <p style="margin-top: 0.75rem;"><strong>Limitation</strong></p>
                  <ul>
                    <li>작은 공간에 물체가 밀집된 환경에서는 성능 저하</li>
                    <li>작은 물체들은 형상만으로 구분이 어려움</li>
                  </ul>

                  <p style="margin-top: 0.75rem;"><strong>Output</strong></p>
                  <ul>
                    <li>Geometry-derived labels (per-point / per-voxel prediction)</li>
                    <li>Floor/Wall 같은 구조 클래스의 anchor 정보</li>
                  </ul>
                <!-- Visual outputs (images) -->
                <div class="columns is-variable is-5" style="margin-top: 1.2rem;">
                  <div class="column">
                    <figure class="image pipeline-figure">
                      <img src="image/step3_pc.jpg" alt="output_pointcloud" class="pipeline-img" />
                      <figcaption class="pipeline-caption">output_pointcloud</figcaption>
                    </figure>
                  </div>
                  <div class="column">
                    <figure class="image pipeline-figure">
                      <img src="image/step3_mesh.jpg" alt="output_mesh" class="pipeline-img" />
                      <figcaption class="pipeline-caption">output_mesh</figcaption>
                    </figure>
                  </div>
                </div>
                </div>
              </article>

              <article class="message is-danger is-light">
                <div class="message-header">
                  <p>Step 4. Open-vocabulary Features (OpenScene)</p>
                </div>
                <div class="message-body">
                  <p><strong>Purpose</strong></p>
                  <ul>
                    <li><strong>RGB 기반 appearance 단서로</strong> 작은 물체 구분 강화</li>
                    <li>텍스트 질의(bed, chair, bag)에 반응하는 feature 생성</li>
                    <li>질의 기반 의미 탐색(query-driven semantic search)까지 확장</li>
                  </ul>

                  <p style="margin-top: 0.75rem;"><strong>What happens (conceptually)</strong></p>
                  <ul>
                    <li>RGB 프레임에서 언어 정렬된 feature 추출</li>
                    <li>3D 공간으로 투영해 point-wise feature로 누적</li>
                    <li>3D 문맥(geometry)을 고려해 feature를 더 일관되게 정제</li>
                  </ul>

                  <p style="margin-top: 0.75rem;"><strong>What it enables</strong></p>
                  <ul>
                    <li>작은 물체(침대, 의자, 책 등) 인식 개선</li>
                    <li>텍스트 질의에 따라 관련 3D 영역을 유연하게 강조</li>
                  </ul>

                  <p style="margin-top: 0.75rem;"><strong>Output</strong></p>
                  <ul>
                    <li>Open-vocabulary 3D features (per-point embedding)</li>
                  </ul>
                <!-- Visual outputs (images) -->
                <div class="columns is-variable is-5" style="margin-top: 1.2rem;">
                  <div class="column">
                    <figure class="image pipeline-figure">
                      <img src="image/step4_pc.jpg" alt="output_pointcloud" class="pipeline-img" />
                      <figcaption class="pipeline-caption">output_pointcloud</figcaption>
                    </figure>
                  </div>
                  <div class="column">
                    <figure class="image pipeline-figure">
                      <img src="image/step4_mesh.jpg" alt="output_mesh" class="pipeline-img" />
                      <figcaption class="pipeline-caption">output_mesh</figcaption>
                    </figure>
                  </div>
                </div>
                </div>
              </article>

              <article class="message is-primary is-light">
                <div class="message-header">
                  <p>Step 5. Fusion: Structure (Geometry) + Objects (Open-vocab)</p>
                </div>
                <div class="message-body">
                  <p><strong>Why fusion</strong></p>
                  <ul>
                    <li>Geometry-only는 구조물에 강함, 작은 물체에 약함</li>
                    <li>Open-vocab는 물체에 강함, 구조적 안정성이 흔들릴 수 있음</li>
                    <li>둘을 결합해 전체적으로 안정성 높은 Semantic map을 목표로 함</li>
                  </ul>

                  <p style="margin-top: 0.75rem;"><strong>Fusion strategy (high-level)</strong></p>
                  <ul>
                    <li>구조적 영역(바닥/벽 등): Mosaic3D anchor를 더 신뢰</li>
                    <li>물체 영역(침대/의자 등): OpenScene feature를 더 신뢰</li>
                    <li>결과적으로 구조는 흔들리지 않고, 물체는 더 잘 분리되며, 질의 기반 탐색도 가능</li>
                  </ul>

                  <p style="margin-top: 0.75rem;"><strong>Output</strong></p>
                  <ul>
                    <li>최종 semantic map (per-point scores / labels)</li>
                    <li>interactive query에 바로 사용할 수 있는 representation</li>
                  </ul>
                </div>
              </article>

              <article class="message is-info is-light">
                <div class="message-header">
                  <p>Step 6. Interactive Demo (viser): Ask the Map</p>
                </div>
                <div class="message-body">
                  <p><strong>Purpose</strong></p>
                  <ul>
                    <li>연구 결과를 직관적으로 이해할 수 있게 시각화</li>
                    <li>로봇처럼 공간이 답하는 느낌</li>
                  </ul>

                  <p style="margin-top: 0.75rem;"><strong>Interaction</strong></p>
                  <ul>
                    <li>텍스트 질의 입력</li>
                    <li>3D 공간에서 관련 영역이 즉시 강조</li>
                  </ul>

                  <p style="margin-top: 0.75rem;"><strong>Example queries</strong></p>
                  <div class="tags">
                    <span class="tag is-dark is-light">Where can I walk?</span>
                    <span class="tag is-dark is-light">bed</span>
                    <span class="tag is-dark is-light">chair</span>
                    <span class="tag is-dark is-light">desk</span>
                    <span class="tag is-dark is-light">wall</span>
                    <span class="tag is-dark is-light">floor</span>
                  </div>
                </div>
              </article>

              <div class="notification is-light" style="margin-top: 1.25rem;">
                <p><strong>Data Transformation Summary</strong></p>
                <p style="margin-top: 0.25rem;">From raw sensor data → to robot-readable map</p>
                <ul style="margin-top: 0.5rem;">
                  <li>Raw depth/RGB/IMU (연속 프레임)</li>
                  <li>Geometry SLAM (이동 경로 복원)</li>
                  <li>3D reconstruction (지도 생성)</li>
                  <li>Geometry segmentation (구조적인 feature 확보)</li>
                  <li>Open-vocabulary features (물체 + 언어 feature 확보)</li>
                  <li>Fusion (구조 안정성 + 물체 인식 + 질의 대응)</li>
                  <li>Interactive demo</li>
                </ul>
              </div>

            </div>
          </div>

        </div>
      </div>
    </div>
  </section>

  <!-- 시작하기 -->
  <section class="section" style="background-color: #fafafa;">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title has-text-centered">시작하기</h2>
          <div class="content">
            <h3>SLAM 파이프라인</h3>
            <pre><code>git clone https://github.com/SLAM-Dunk-Prometheus/SLAM-Pipeline.git
cd SLAM-Pipeline
# 저장소 README의 설치 지침을 따르세요

# 환경 설정
conda create -n slam-pipeline python=3.9
conda activate slam-pipeline
pip install -r requirements.txt

# SLAM 실행
python LiDAR_IMU_SLAM.py data/

# 데이터 전처리
python post_processor.py data/ --integrate</code></pre>

            <h3>3D 세그멘테이션</h3>
            <pre><code>git clone https://github.com/SLAM-Dunk-Prometheus/Segmentation.git
cd Segmentation
# 저장소 README의 설치 지침을 따르세요

# StrayScanner 데이터 변환
python convert_stray.py

# 세그멘테이션 데모 실행
cd mosaic3d
# 데모 코드 실행</code></pre>

            <h3>데이터 구조</h3>
            <p>StrayScanner 데이터는 다음과 같은 구조로 구성됩니다.</p>
            <pre><code>data/
├── rgb.mp4              # RGB 영상
├── camera_matrix.csv    # 카메라 행렬
├── odometry.csv         # 오도메트리 데이터
├── imu.csv              # IMU 데이터
├── depth/               # Depth 맵
│   ├── 000000.npy
│   ├── 000001.npy
│   └── ...
└── confidence/          # Depth confidence 맵
    ├── 000000.png
    └── ...</code></pre>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Footer -->
  <footer>
    <div class="container">
      <div class="content has-text-centered">
        <p>
          <strong>SLAM-Dunk-Prometheus</strong> by Prometheus Team
        </p>
        <p>
          <a href="https://github.com/SLAM-Dunk-Prometheus">
            <i class="fab fa-github fa-2x"></i>
          </a>
        </p>
        <p style="margin-top: 1rem; font-size: 0.9rem; color: #888;">
          웹사이트 템플릿은 <a href="https://nerfies.github.io/">Nerfies</a>를 기반으로 합니다.
        </p>
      </div>
    </div>
  </footer>

</body>
</html>
